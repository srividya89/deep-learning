{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6146216a-0b6f-4651-85f5-cc80b639d220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: (60000, 28, 28) (60000,)\n",
      "Test samples: (10000, 28, 28) (10000,)\n",
      "Batch X (Training) shapes: (60000, 28, 28) (60000,)\n",
      "Batch Y (Testing) shapes: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function\n",
    "# Import MNIST data\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize values between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(\"Training samples:\", x_train.shape, y_train.shape)\n",
    "print(\"Test samples:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# ----------------------------------\n",
    "# Define batch_x as the full training dataset\n",
    "# Define batch_y as the full testing dataset\n",
    "# ----------------------------------\n",
    "batch_x = (x_train, y_train)   # Training data (features + labels)\n",
    "batch_y = (x_test, y_test)     # Testing data (features + labels)\n",
    "\n",
    "# Check the shapes\n",
    "print(\"Batch X (Training) shapes:\", batch_x[0].shape, batch_x[1].shape)\n",
    "print(\"Batch Y (Testing) shapes:\", batch_y[0].shape, batch_y[1].shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdd51bfd-485e-4ee1-b82d-a70e7af2331f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: (60000, 28, 28) (60000, 10)\n",
      "Test samples: (10000, 28, 28) (10000, 10)\n",
      "Batch X (Training) shapes: (60000, 28, 28) (60000, 10)\n",
      "Batch Y (Testing) shapes: (10000, 28, 28) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize values between 0 and 1\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# One-hot encode labels (10 classes for digits 0–9)\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"Training samples:\", x_train.shape, y_train.shape)\n",
    "print(\"Test samples:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# ----------------------------------\n",
    "# Define batch_x as the full training dataset\n",
    "# Define batch_y as the full testing dataset\n",
    "# ----------------------------------\n",
    "batch_x = (x_train, y_train)   # Training data (features + one-hot labels)\n",
    "batch_y = (x_test, y_test)     # Testing data (features + one-hot labels)\n",
    "\n",
    "# Check the shapes\n",
    "print(\"Batch X (Training) shapes:\", batch_x[0].shape, batch_x[1].shape)\n",
    "print(\"Batch Y (Testing) shapes:\", batch_y[0].shape, batch_y[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98208b4e-d5a9-4a95-a934-e38f6de13f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00243f20-2ded-42a4-a21e-e74672fb2a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: (60000, 784) (60000, 10)\n",
      "Test samples: (10000, 784) (10000, 10)\n",
      "Step 1, Minibatch Loss= 3214.6204, Training Accuracy= 0.078\n",
      "Step 100, Minibatch Loss= 270.7203, Training Accuracy= 0.727\n",
      "Step 200, Minibatch Loss= 270.2548, Training Accuracy= 0.797\n",
      "Step 300, Minibatch Loss= 131.1882, Training Accuracy= 0.859\n",
      "Step 400, Minibatch Loss= 145.7529, Training Accuracy= 0.875\n",
      "Step 500, Minibatch Loss= 94.8467, Training Accuracy= 0.867\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8576\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# ---------------------------\n",
    "# Load and preprocess MNIST\n",
    "# ---------------------------\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Flatten images: (28,28) → (784,)\n",
    "x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "print(\"Training samples:\", x_train.shape, y_train.shape)\n",
    "print(\"Test samples:\", x_test.shape, y_test.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# Training Parameters\n",
    "# ---------------------------\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 128\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256   # 1st layer number of neurons\n",
    "n_hidden_2 = 256   # 2nd layer number of neurons\n",
    "num_input  = 784   # MNIST data input (28*28 pixels)\n",
    "num_classes = 10   # MNIST total classes (0–9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, num_input])\n",
    "y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}\n",
    "\n",
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "logits = neural_net(x)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "    logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# ---------------------------\n",
    "# Training\n",
    "# ---------------------------\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        # Get a batch manually\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        batch_x, batch_y = x_train[idx], y_train[idx]\n",
    "\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={x: batch_x, y: batch_y})\n",
    "\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy],\n",
    "                                 feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Step \" + str(step) +\n",
    "                  \", Minibatch Loss= {:.4f}\".format(loss) +\n",
    "                  \", Training Accuracy= {:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\",\n",
    "          sess.run(accuracy, feed_dict={x: x_test, y: y_test}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c992f2-7019-4d3a-9850-64eebae7030a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
